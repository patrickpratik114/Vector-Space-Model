{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "OR4L0PiDFzp-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import logging\n",
        "import math\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_text_files(folder_path):\n",
        "    data = {}\n",
        "    doc_id_to_filename = {}\n",
        "    doc_id = 0\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
        "                data[doc_id] = file.read()\n",
        "                doc_id_to_filename[doc_id] = filename\n",
        "                logging.info(f\"Loaded file: {filename} with doc_id: {doc_id}\")\n",
        "                doc_id += 1\n",
        "    return data, doc_id_to_filename\n"
      ],
      "metadata": {
        "id": "bMHq7vEoGsBY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "    return text.lower().split()"
      ],
      "metadata": {
        "id": "Oqg22P8BGyNK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def term_frequency(term, document):\n",
        "    return document.count(term) / len(document)"
      ],
      "metadata": {
        "id": "Gw-1-tYDG5ss"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inverse_document_frequency(term, all_documents):\n",
        "    num_docs_containing_term = sum(1 for doc in all_documents if term in doc)\n",
        "    return math.log(len(all_documents) / (1 + num_docs_containing_term))"
      ],
      "metadata": {
        "id": "qJP24Il2JKf7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_tfidf(document, all_documents, vocab):\n",
        "    tfidf_vector = []\n",
        "    for term in vocab:\n",
        "        tf = term_frequency(term, document)\n",
        "        idf = inverse_document_frequency(term, all_documents)\n",
        "        tfidf_vector.append(tf * idf)\n",
        "    return np.array(tfidf_vector)"
      ],
      "metadata": {
        "id": "9erh72LwJNgK"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(vec1, vec2):\n",
        "    dot_product = np.dot(vec1, vec2)\n",
        "    norm_vec1 = np.linalg.norm(vec1)\n",
        "    norm_vec2 = np.linalg.norm(vec2)\n",
        "    return dot_product / (norm_vec1 * norm_vec2) if norm_vec1 * norm_vec2 != 0 else 0"
      ],
      "metadata": {
        "id": "c5WO-N_ZJQGZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    folder_path = \"/content/drive/MyDrive/Vector Space Model/Docs\"\n",
        "\n",
        "    docs, doc_id_to_filename = load_text_files(folder_path)\n",
        "\n",
        "    queries = [\"computer science\",\n",
        "               \"web development\",\n",
        "               \"machine learning\",\n",
        "               \"django\",\n",
        "               \"data science\",\n",
        "               \"problem\"]\n",
        "\n",
        "    tokenized_docs = [tokenize(doc) for doc in docs.values()]\n",
        "\n",
        "    vocab = sorted(set(word for doc in tokenized_docs for word in doc))\n",
        "    print(logging.info(f\"Vocabulary size: {len(vocab)}\"))\n",
        "\n",
        "    doc_tfidf_vectors = [compute_tfidf(doc, tokenized_docs, vocab) for doc in tokenized_docs]\n",
        "\n",
        "    results = []\n",
        "    for query in queries:\n",
        "        tokenized_query = tokenize(query)\n",
        "        query_tfidf_vector = compute_tfidf(tokenized_query, tokenized_docs, vocab)\n",
        "\n",
        "        similarities = []\n",
        "        for doc_id, doc_vector in enumerate(doc_tfidf_vectors):\n",
        "            similarity = cosine_similarity(query_tfidf_vector, doc_vector)\n",
        "            similarities.append((doc_id, similarity))\n",
        "\n",
        "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        results.append((query, similarities))\n",
        "\n",
        "        print(results)\n",
        "\n",
        "    path = \"/content/drive/MyDrive/Vector Space Model/result\"\n",
        "    output_file = os.path.join(path, \"results_Pratik_Shrestha.txt\")\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        for query, similarities in results:\n",
        "            f.write(f\"Query: {query}\\n\")\n",
        "            for doc_id, similarity in similarities:\n",
        "                filename = doc_id_to_filename[doc_id]\n",
        "                f.write(f\"  Document: {filename}, Similarity: {similarity:.4f}\\n\")\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "    logging.info(f\"Results written to {output_file}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpCEypFnJS8d",
        "outputId": "d76b6b2e-1523-4b65-c279-574ad50b9b39"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "[('computer science', [(0, 0.3149851856359772), (5, 0.24941470850596803), (6, 0.13364159033361334), (8, 0.11047418181171996), (1, 0.0), (2, 0.0), (3, 0.0), (4, 0.0), (7, 0.0), (9, 0.0)])]\n",
            "[('computer science', [(0, 0.3149851856359772), (5, 0.24941470850596803), (6, 0.13364159033361334), (8, 0.11047418181171996), (1, 0.0), (2, 0.0), (3, 0.0), (4, 0.0), (7, 0.0), (9, 0.0)]), ('web development', [(9, 0.5601607721465391), (2, 0.13320895565608798), (0, 0.0), (1, 0.0), (3, 0.0), (4, 0.0), (5, 0.0), (6, 0.0), (7, 0.0), (8, 0.0)])]\n",
            "[('computer science', [(0, 0.3149851856359772), (5, 0.24941470850596803), (6, 0.13364159033361334), (8, 0.11047418181171996), (1, 0.0), (2, 0.0), (3, 0.0), (4, 0.0), (7, 0.0), (9, 0.0)]), ('web development', [(9, 0.5601607721465391), (2, 0.13320895565608798), (0, 0.0), (1, 0.0), (3, 0.0), (4, 0.0), (5, 0.0), (6, 0.0), (7, 0.0), (8, 0.0)]), ('machine learning', [(1, 0.35227499247565675), (8, 0.14357589497578502), (0, 0.0), (2, 0.0), (3, 0.0), (4, 0.0), (5, 0.0), (6, 0.0), (7, 0.0), (9, 0.0)])]\n",
            "[('computer science', [(0, 0.3149851856359772), (5, 0.24941470850596803), (6, 0.13364159033361334), (8, 0.11047418181171996), (1, 0.0), (2, 0.0), (3, 0.0), (4, 0.0), (7, 0.0), (9, 0.0)]), ('web development', [(9, 0.5601607721465391), (2, 0.13320895565608798), (0, 0.0), (1, 0.0), (3, 0.0), (4, 0.0), (5, 0.0), (6, 0.0), (7, 0.0), (8, 0.0)]), ('machine learning', [(1, 0.35227499247565675), (8, 0.14357589497578502), (0, 0.0), (2, 0.0), (3, 0.0), (4, 0.0), (5, 0.0), (6, 0.0), (7, 0.0), (9, 0.0)]), ('django', [(9, 0.3300994886534443), (0, 0.0), (1, 0.0), (2, 0.0), (3, 0.0), (4, 0.0), (5, 0.0), (6, 0.0), (7, 0.0), (8, 0.0)])]\n",
            "[('computer science', [(0, 0.3149851856359772), (5, 0.24941470850596803), (6, 0.13364159033361334), (8, 0.11047418181171996), (1, 0.0), (2, 0.0), (3, 0.0), (4, 0.0), (7, 0.0), (9, 0.0)]), ('web development', [(9, 0.5601607721465391), (2, 0.13320895565608798), (0, 0.0), (1, 0.0), (3, 0.0), (4, 0.0), (5, 0.0), (6, 0.0), (7, 0.0), (8, 0.0)]), ('machine learning', [(1, 0.35227499247565675), (8, 0.14357589497578502), (0, 0.0), (2, 0.0), (3, 0.0), (4, 0.0), (5, 0.0), (6, 0.0), (7, 0.0), (9, 0.0)]), ('django', [(9, 0.3300994886534443), (0, 0.0), (1, 0.0), (2, 0.0), (3, 0.0), (4, 0.0), (5, 0.0), (6, 0.0), (7, 0.0), (8, 0.0)]), ('data science', [(6, 0.4681129990351264), (8, 0.2579755819763389), (0, 0.134887691305369), (2, 0.11967418996634299), (1, 0.0), (3, 0.0), (4, 0.0), (5, 0.0), (7, 0.0), (9, 0.0)])]\n",
            "[('computer science', [(0, 0.3149851856359772), (5, 0.24941470850596803), (6, 0.13364159033361334), (8, 0.11047418181171996), (1, 0.0), (2, 0.0), (3, 0.0), (4, 0.0), (7, 0.0), (9, 0.0)]), ('web development', [(9, 0.5601607721465391), (2, 0.13320895565608798), (0, 0.0), (1, 0.0), (3, 0.0), (4, 0.0), (5, 0.0), (6, 0.0), (7, 0.0), (8, 0.0)]), ('machine learning', [(1, 0.35227499247565675), (8, 0.14357589497578502), (0, 0.0), (2, 0.0), (3, 0.0), (4, 0.0), (5, 0.0), (6, 0.0), (7, 0.0), (9, 0.0)]), ('django', [(9, 0.3300994886534443), (0, 0.0), (1, 0.0), (2, 0.0), (3, 0.0), (4, 0.0), (5, 0.0), (6, 0.0), (7, 0.0), (8, 0.0)]), ('data science', [(6, 0.4681129990351264), (8, 0.2579755819763389), (0, 0.134887691305369), (2, 0.11967418996634299), (1, 0.0), (3, 0.0), (4, 0.0), (5, 0.0), (7, 0.0), (9, 0.0)]), ('problem', [(5, 0.3134306671457341), (6, 0.28995314761145496), (0, 0.0), (1, 0.0), (2, 0.0), (3, 0.0), (4, 0.0), (7, 0.0), (8, 0.0), (9, 0.0)])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZnwGz6O_KVxJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}